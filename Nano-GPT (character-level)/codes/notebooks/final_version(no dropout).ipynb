{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "51cdeb81-be63-4d63-bd6a-3b4375513da6",
    "_uuid": "f5b85664-7fd5-475e-ba60-4c2a5d403537",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-12T08:39:40.262146Z",
     "iopub.status.busy": "2024-05-12T08:39:40.261349Z",
     "iopub.status.idle": "2024-05-12T08:39:41.966408Z",
     "shell.execute_reply": "2024-05-12T08:39:41.965414Z",
     "shell.execute_reply.started": "2024-05-12T08:39:40.262111Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "e3d53b15-522f-4268-9012-ca5853cd3600",
    "_uuid": "ca9867d0-d523-4ec3-92c3-01a544af33e0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-12T08:39:41.968833Z",
     "iopub.status.busy": "2024-05-12T08:39:41.968429Z",
     "iopub.status.idle": "2024-05-12T08:39:42.001165Z",
     "shell.execute_reply": "2024-05-12T08:39:42.000231Z",
     "shell.execute_reply.started": "2024-05-12T08:39:41.968807Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() == True else 'cpu'\n",
    "# device = 'cpu'\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "ec979244-06b6-4d35-a07a-14351387f5b5",
    "_uuid": "ece399af-6f05-422e-b340-d5d4b67c7bf3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-12T08:39:42.002714Z",
     "iopub.status.busy": "2024-05-12T08:39:42.002355Z",
     "iopub.status.idle": "2024-05-12T08:39:42.013219Z",
     "shell.execute_reply": "2024-05-12T08:39:42.012403Z",
     "shell.execute_reply.started": "2024-05-12T08:39:42.002685Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Class GetBatch : loader\n",
    "class GetBatch:\n",
    "    def __init__(self, data, batch_size, block_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def get_batch(self):\n",
    "        # generate a small batch of data of input x and targets 7\n",
    "        # get_batch serves as a dataloader\n",
    "        ix = torch.randint(len(self.data) - self.block_size, (self.batch_size,))\n",
    "        x = torch.stack([self.data[i:i + self.block_size] for i in ix], dim=0)\n",
    "        y = torch.stack([self.data[i + 1:i + self.block_size + 1] for i in ix], dim=0)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "f54772d0-e822-4a75-8a0c-0c6f6e9b5221",
    "_uuid": "ee03c1d1-8b3b-43ec-8ce0-965c742fd69f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-12T08:39:42.014461Z",
     "iopub.status.busy": "2024-05-12T08:39:42.014207Z",
     "iopub.status.idle": "2024-05-12T08:39:42.023848Z",
     "shell.execute_reply": "2024-05-12T08:39:42.022941Z",
     "shell.execute_reply.started": "2024-05-12T08:39:42.014417Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Function guess_loss\n",
    "@torch.no_grad()\n",
    "def guess_loss(model, loader, eval_iters=None):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    if eval_iters is None:\n",
    "        eval_iters = 100\n",
    "    losses = torch.zeros(eval_iters, dtype=torch.float32).to(device)\n",
    "    for k in range(eval_iters):\n",
    "        xb, yb = loader.get_batch()\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        yb = yb.flatten(end_dim=1)\n",
    "        logits = model(xb, yb)\n",
    "        logits = logits.flatten(end_dim=1)\n",
    "        loss = loss_fn(logits,yb)\n",
    "        losses[k] = loss.item()\n",
    "    avg_loss = losses.mean().item()\n",
    "    model.train()\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "e977ff75-4116-47af-a3d0-ed4260a43674",
    "_uuid": "479b48b1-de25-4b48-b9ea-51586c2f3ef8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-12T08:39:42.027277Z",
     "iopub.status.busy": "2024-05-12T08:39:42.026808Z",
     "iopub.status.idle": "2024-05-12T08:39:42.055091Z",
     "shell.execute_reply": "2024-05-12T08:39:42.054411Z",
     "shell.execute_reply.started": "2024-05-12T08:39:42.027254Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create Loaders and datasets\n",
    "file_name = 'shakespeare.txt'\n",
    "file = open(file_name)\n",
    "text = file.read()\n",
    "text = text.replace('\"','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "block_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "e977ff75-4116-47af-a3d0-ed4260a43674",
    "_uuid": "479b48b1-de25-4b48-b9ea-51586c2f3ef8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-12T08:39:42.056227Z",
     "iopub.status.busy": "2024-05-12T08:39:42.056002Z",
     "iopub.status.idle": "2024-05-12T08:39:43.899215Z",
     "shell.execute_reply": "2024-05-12T08:39:43.898011Z",
     "shell.execute_reply.started": "2024-05-12T08:39:42.056207Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "chars = sorted(set(list(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "stoi = {s: i for i, s in enumerate(chars)}\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text))\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "data_train = data[:n]\n",
    "data_test = data[n:]\n",
    "loader_train = GetBatch(data_train, batch_size, block_size)\n",
    "loader_test = GetBatch(data_test, batch_size, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "5e7da2c5-4655-4794-9f8e-b9cb5a89fe0c",
    "_uuid": "a9ba4dc9-169c-4925-aa1a-652a12452e9e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-12T08:39:43.902315Z",
     "iopub.status.busy": "2024-05-12T08:39:43.900751Z",
     "iopub.status.idle": "2024-05-12T08:39:43.914205Z",
     "shell.execute_reply": "2024-05-12T08:39:43.912913Z",
     "shell.execute_reply.started": "2024-05-12T08:39:43.902279Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Class Head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, block_size, n_emb, head_size=None):\n",
    "        super(Head, self).__init__()\n",
    "\n",
    "        if head_size is None:\n",
    "            head_size = n_emb\n",
    "\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)).to(device))\n",
    "        \n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        wei = k @ q.permute(0, -1, -2) * C ** -0.5\n",
    "#         wei = self.dropout(wei)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, value=float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T08:39:43.916749Z",
     "iopub.status.busy": "2024-05-12T08:39:43.916321Z",
     "iopub.status.idle": "2024-05-12T08:39:43.928503Z",
     "shell.execute_reply": "2024-05-12T08:39:43.927506Z",
     "shell.execute_reply.started": "2024-05-12T08:39:43.916713Z"
    }
   },
   "outputs": [],
   "source": [
    "# Class MultiHeadAttention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, block_size, n_emb, head_size=None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        if head_size is None:\n",
    "            head_size = 32\n",
    "        self.heads = nn.ModuleList([Head(block_size, n_emb, head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads*head_size,num_heads*head_size)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "#         out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T08:39:43.930264Z",
     "iopub.status.busy": "2024-05-12T08:39:43.929849Z",
     "iopub.status.idle": "2024-05-12T08:39:43.937244Z",
     "shell.execute_reply": "2024-05-12T08:39:43.936101Z",
     "shell.execute_reply.started": "2024-05-12T08:39:43.930234Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,num_f):\n",
    "        super(FeedForward,self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=num_f, out_features=4*num_f),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=4*num_f, out_features=num_f),\n",
    "#             nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "4fc3d13c-d605-4d4e-9367-0e66e839268d",
    "_uuid": "170525f1-82b6-49eb-a55d-c0ee0320ab6d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-12T08:39:43.938899Z",
     "iopub.status.busy": "2024-05-12T08:39:43.938551Z",
     "iopub.status.idle": "2024-05-12T08:39:43.949619Z",
     "shell.execute_reply": "2024-05-12T08:39:43.948668Z",
     "shell.execute_reply.started": "2024-05-12T08:39:43.938869Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# FeedForward Class\n",
    "class Block(nn.Module):\n",
    "    \"\"\" a simple linear model followed by a non_linearity \"\"\"\n",
    "\n",
    "    def __init__(self, block_size, num_heads, head_size, n_emb):\n",
    "        super(Block, self).__init__()\n",
    "        self.sa_head = MultiHeadAttention(num_heads=num_heads, \n",
    "                        block_size=block_size, n_emb=n_emb, head_size=head_size)\n",
    "        mlp_f = num_heads*head_size\n",
    "        self.ffwd = FeedForward(mlp_f)\n",
    "        self.ln1 = nn.LayerNorm(num_heads*head_size)\n",
    "        self.ln2 = nn.LayerNorm(num_heads*head_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_head(self.ln1(x))\n",
    "        out = x + self.ffwd(self.ln2(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "0a12e2d8-34a4-4c39-bf8c-bec598a094b7",
    "_uuid": "673f3908-b121-4f19-9109-8e3b8acff97d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-12T08:39:43.951026Z",
     "iopub.status.busy": "2024-05-12T08:39:43.950714Z",
     "iopub.status.idle": "2024-05-12T08:39:43.962639Z",
     "shell.execute_reply": "2024-05-12T08:39:43.961854Z",
     "shell.execute_reply.started": "2024-05-12T08:39:43.951004Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class NanoGPT(nn.Module):\n",
    "    def __init__(self, block_size, vocab_size, n_emb, num_heads, head_size):\n",
    "        super(NanoGPT, self).__init__()\n",
    "        self.block_size = block_size\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(block_size, num_heads, head_size, n_emb),\n",
    "            Block(block_size, num_heads, head_size, num_heads*head_size),\n",
    "            Block(block_size, num_heads, head_size, num_heads*head_size),\n",
    "            nn.LayerNorm(num_heads*head_size)\n",
    "        )\n",
    "        \n",
    "        self.lm_head = nn.Linear(num_heads*head_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        if T > self.block_size:\n",
    "            idx = idx[:,-block_size:]\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,S)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(self.block_size, device=device))\n",
    "        x = tok_emb + pos_emb[:T]\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens=None):\n",
    "        \n",
    "        if max_new_tokens is None:\n",
    "            max_new_tokens = 100\n",
    "        for i in range(max_new_tokens):\n",
    "\n",
    "            logits = self(idx)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs[:,-1,:], num_samples=1, replacement=True)\n",
    "            idx = torch.cat([idx, idx_next], dim=-1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "86a186d3-efbe-46dd-b538-8858f6b58441",
    "_uuid": "cebdbfbb-246c-4e9d-a4bd-11e9253dea7f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-12T08:39:43.964070Z",
     "iopub.status.busy": "2024-05-12T08:39:43.963807Z",
     "iopub.status.idle": "2024-05-12T08:39:44.149550Z",
     "shell.execute_reply": "2024-05-12T08:39:44.148612Z",
     "shell.execute_reply.started": "2024-05-12T08:39:43.964042Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram's device: cuda\n"
     ]
    }
   ],
   "source": [
    "dropout = 0.1\n",
    "bigram = NanoGPT(block_size=block_size,vocab_size=vocab_size,n_emb=384,num_heads=6,head_size=64).to(device)\n",
    "print(f\"bigram's device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 5478221\n"
     ]
    }
   ],
   "source": [
    "num_params = 0\n",
    "for params in bigram.parameters():\n",
    "    num_params += torch.numel(params)\n",
    "print(f'number of parameters: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "81f73998-b15a-4022-aa73-e23592b7810e",
    "_uuid": "9bff5df5-c3b2-4029-b406-20faca487da5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-12T08:39:44.151072Z",
     "iopub.status.busy": "2024-05-12T08:39:44.150784Z",
     "iopub.status.idle": "2024-05-12T08:39:46.941335Z",
     "shell.execute_reply": "2024-05-12T08:39:46.940258Z",
     "shell.execute_reply.started": "2024-05-12T08:39:44.151043Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text before optimization --> \n",
      "TxJqIi4we!KZTm\tSwqzXWGEaThiQwupjYLYAluo,5wVG7[ x3S\tqNu'o5mE,\tK3'0Xz:84OgToP,:[smI$r8m\n",
      "),glzH\n",
      "s3Ruvy89DmUz9zkwiNMuK)cp\tPhT5RLjG2sK\n",
      "UsIj:A UzuHod3aKQ:tp7ygVjAN]azCOT9n6D'fLCLz,]pYB9N.$ 1[pJ,CoJJBgQJ\t'1GncADl9DuuoOK7!mywd':]SqeiK!W?Y.78jK(0sQXT\n",
      "oNIMFLAKg\n",
      "j!0XT)mP\t)NgTgr9w,:LC6.9]t9R2p]O6j491X97A0T 0b\n",
      "[\n"
     ]
    }
   ],
   "source": [
    "xb,yb = loader_train.get_batch()\n",
    "xb = xb.to(device)\n",
    "yb = yb.to(device)\n",
    "out = bigram(xb)\n",
    "idx = torch.tensor([[1]],device=device)\n",
    "idx_gen = bigram.generate(idx,max_new_tokens=300)\n",
    "print(f'''Generated text before optimization --> \\n{decode(idx_gen[0].tolist())[1:]}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "419acb5c-5bac-45f6-917d-c45fc054fb76",
    "_uuid": "4da0e755-df05-4324-8903-ebce54f38865",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-12T08:39:46.945460Z",
     "iopub.status.busy": "2024-05-12T08:39:46.945112Z",
     "iopub.status.idle": "2024-05-12T08:39:48.381637Z",
     "shell.execute_reply": "2024-05-12T08:39:48.380660Z",
     "shell.execute_reply.started": "2024-05-12T08:39:46.945412Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(bigram.parameters(),lr=0.009)\n",
    "schedular = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "2dea3277-a49c-4ab2-b855-11f6cc02023a",
    "_uuid": "cddb6faf-64d3-4f45-9f61-3d9ab42c2475",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-12T08:39:48.383286Z",
     "iopub.status.busy": "2024-05-12T08:39:48.382923Z",
     "iopub.status.idle": "2024-05-12T08:39:48.390539Z",
     "shell.execute_reply": "2024-05-12T08:39:48.389639Z",
     "shell.execute_reply.started": "2024-05-12T08:39:48.383261Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()['param_groups'][0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "c31e5835-810c-4c58-92b4-4e967ae2d998",
    "_uuid": "510b1bbb-0276-4e69-9211-860654d7ed13",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-12T09:08:42.382275Z",
     "iopub.status.busy": "2024-05-12T09:08:42.381579Z",
     "iopub.status.idle": "2024-05-12T09:17:08.612643Z",
     "shell.execute_reply": "2024-05-12T09:17:08.611819Z",
     "shell.execute_reply.started": "2024-05-12T09:08:42.382245Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = 1.31386137008667\n",
      "epoch 100: loss = 1.3089337348937988\n",
      "epoch 200: loss = 1.3150941133499146\n",
      "epoch 300: loss = 1.3041549921035767\n",
      "epoch 400: loss = 1.294015645980835\n",
      "epoch 500: loss = 1.2830084562301636\n",
      "epoch 600: loss = 1.2857943773269653\n",
      "epoch 700: loss = 1.2819838523864746\n",
      "epoch 800: loss = 1.2783098220825195\n",
      "epoch 900: loss = 1.2784326076507568\n"
     ]
    }
   ],
   "source": [
    "bigram.train()\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    xb, yb = loader_train.get_batch()\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    yb = yb.flatten(end_dim=1)\n",
    "    logits = bigram(xb)\n",
    "    logits = logits.flatten(end_dim=1)\n",
    "    loss = loss_fn(logits, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'''epoch {epoch}: loss = {loss.item()}''')\n",
    "        schedular.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "934452ba-5606-4c30-a9b7-549c68075010",
    "_uuid": "700501e3-6c60-41f1-aa06-82f1c6c63711",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-12T09:17:50.938473Z",
     "iopub.status.busy": "2024-05-12T09:17:50.937584Z",
     "iopub.status.idle": "2024-05-12T09:18:02.262170Z",
     "shell.execute_reply": "2024-05-12T09:18:02.261233Z",
     "shell.execute_reply.started": "2024-05-12T09:17:50.938442Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text after optimization --> \n",
      "Where's hath show'd with createst sell.\n",
      "When they are before, but to-day?\n",
      "Now can I not of her learned in you befriends?\n",
      "He is a few in us in honey-house, about of awn,\n",
      "To see wind the offer, I took you.\n",
      "I hear further he chair\n",
      "He best, is an endure unto successity\n",
      "A tables? Tribunes in Quarvellous in,\n",
      "And not mark the king! What our dear man?\n",
      "To-morrow, you hear that has boy, careers,\n",
      "God, first is out or to strong upon him.\n",
      "SILV\n",
      "I have lady, too cry that you hurtle: sure, must\n",
      "If any all first infirmitie, and to your majesty,\n",
      "I saw here is more profits fair nothing rough,\n",
      "Do we willingly work.\n",
      "The was good disputal bought captain shall be,\n",
      "Out only Guildens there comes ere words,\n",
      "That seek up.\n",
      "Who's Murder doth Antony is too her: the face?\n",
      "I Aleveller, of the horship content:\n",
      "The more more was where worth that must fortunes,\n",
      "Henry all the portiverance honourable,\n",
      "This spirit and shield not Timon:\n",
      "And hath by the King on, the letter'd it on\n",
      "Shall be true convenitied the less me.\n",
      "They be cried the ran with that Christian sod glory\n",
      "Who washes, and be those wondrous else eyes,\n",
      "As good on that vengeance and batt than great.\n",
      "Lay dhy being that I have laid from Troy\n",
      "How o'er I, with thee a dove a most afoot\n",
      "But, on thyself at office, from five her\n",
      "Must dovers, an evil think I will make perforce.\n",
      "Dashephanor cross Troy and swallows, yet stirrow.\n",
      "Whether dear this with the promises noble ear,\n",
      "Or I say-in the Canor Gloucester, Duke of York,\n",
      "For Will have knot they so savage your blood,\n",
      "And my father's wear at that did look,\n",
      "Which bred, so injury well-diving and now thus:\n",
      "Go some thy land lend hand the kiss upon thors,\n",
      "And thy much,\n",
      "And why can I have gazenge as messenger\n",
      "The water-how? And small burnt art, I.\n",
      "There like a fool invital come of man.\n",
      "Wrapp'd sir, Somerset, gracious well. Villain!\n",
      "So I take him, I were:\n",
      "My well does did end me that art my lord,\n",
      "Stand be undes and all then. The let thee, away of high choose, they are mean\n",
      "have it base a simple practisely village\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    bigram.eval()\n",
    "    idx = torch.tensor([[0]],device=device)\n",
    "    idx_gen = bigram.generate(idx,max_new_tokens=2000)\n",
    "    print(f'''Generated text after optimization --> \\n{decode(idx_gen[0].tolist())[1:]}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "c5b8ff36-2fc7-4529-9ed6-87e6eb6bbe62",
    "_uuid": "82a4db18-754b-4966-907e-9258ca0ecffb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-12T09:17:15.614767Z",
     "iopub.status.busy": "2024-05-12T09:17:15.613935Z",
     "iopub.status.idle": "2024-05-12T09:17:46.987122Z",
     "shell.execute_reply": "2024-05-12T09:17:46.986112Z",
     "shell.execute_reply.started": "2024-05-12T09:17:15.614736Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_train = 1.2682071924209595\n",
      "loss_test = 1.4151519536972046\n"
     ]
    }
   ],
   "source": [
    "loss_train = guess_loss(bigram,loader_train,eval_iters=100)\n",
    "print(f'loss_train = {loss_train}')\n",
    "loss_test = guess_loss(bigram,loader_test,eval_iters=100)\n",
    "print(f'loss_test = {loss_test}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1028,
     "sourceId": 2124,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4019756,
     "sourceId": 6993633,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python [conda env:torch-cuda]",
   "language": "python",
   "name": "conda-env-torch-cuda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
