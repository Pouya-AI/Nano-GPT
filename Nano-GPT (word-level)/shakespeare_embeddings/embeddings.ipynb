{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import numpy.random as random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # getting shakespeare content from website (already dotorch.nn.functionalmport requests\n",
    "# response = requests.get('https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt')\n",
    "\n",
    "# if response.status_code == 200:\n",
    "#     text = response.text\n",
    "# with open('shakespeare.txt','w') as file:\n",
    "#     file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open downloaded text file\n",
    "# with open('shakespeare.txt','r') as file:\n",
    "#     text = file.read()[10462:-569]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tokenizer and loading vocabs and merges \n",
    "from tokenizers.tokenizer import Tokenizer\n",
    "tok = Tokenizer()\n",
    "tok.vocabs = tok.load('tokenizers/vocabs.pkl')\n",
    "tok.merges = tok.load('tokenizers/merges.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have ncoded text file, no need from encoding\n",
    "with open('tokens.pkl','rb') as file:\n",
    "    tokens = pickle.load(file)\n",
    "    \n",
    "tokens = torch.tensor(tokens)\n",
    "batch_size = 4\n",
    "k = 5\n",
    "text = ''.join([tok.decode([token.item()]) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self,tokens,batch_size,k,return_tensor=True):\n",
    "        \n",
    "        self.tokens = tokens\n",
    "        self.num_tokens = len(tokens)\n",
    "        self.batch_size = batch_size\n",
    "        self.k = k\n",
    "        self.return_tensor = return_tensor\n",
    "        \n",
    "        self.negative_keys,self.negative_values = self.get_distribution()\n",
    "        self.ones = np.ones((batch_size,1),dtype=int)\n",
    "        self.lbls = np.concatenate(((np.ones((batch_size,2))),np.zeros((batch_size,k))),axis=1)\n",
    "        if return_tensor == True:\n",
    "            self.lbls = torch.tensor(self.lbls)\n",
    "        \n",
    "    def get_distribution(self):\n",
    "        occurance_dictionary = {}\n",
    "        for token in self.tokens:\n",
    "            occurance_dictionary[token] = occurance_dictionary.get(token,0)+1\n",
    "\n",
    "        values = list(occurance_dictionary.values())\n",
    "        keys = list(occurance_dictionary.keys())\n",
    "\n",
    "        values = [value**(3/4) for value in values]\n",
    "        values = [value/sum(values) for value in values]\n",
    "\n",
    "        return keys,values\n",
    "    \n",
    "    def get_targets_negative(self):\n",
    "        \n",
    "        targets_negative = self.ones@random.choice(self.negative_keys,\n",
    "                                    size=(1,self.k),p=self.negative_values)\n",
    "        \n",
    "        return targets_negative\n",
    "    \n",
    "    def get_context_ids(self):\n",
    "        \n",
    "        idx_context = random.randint(1,self.num_tokens-1,(self.batch_size,1))\n",
    "        ids_targets_positive = np.concatenate((idx_context-1,idx_context+1),1)\n",
    "        \n",
    "        return idx_context,ids_targets_positive\n",
    "    \n",
    "    def load_batch(self):\n",
    "        \n",
    "        targets_negative = self.get_targets_negative()\n",
    "        idx_context,ids_targets_positive = self.get_context_ids()\n",
    "        \n",
    "        context = self.tokens[idx_context]\n",
    "        targets_positive = self.tokens[ids_targets_positive]\n",
    "        \n",
    "        targets = np.concatenate((targets_positive,targets_negative),axis=-1)\n",
    "        \n",
    "        if self.return_tensor == True:\n",
    "            context = torch.tensor(context)\n",
    "            targets = torch.tensor(targets)\n",
    "            \n",
    "        return (context,targets,self.lbls),(idx_context,ids_targets_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self,tokens,batch_size,k):\n",
    "        \n",
    "        self.tokens = tokens\n",
    "        self.num_tokens = len(tokens)\n",
    "        self.batch_size = batch_size\n",
    "        self.k = k\n",
    "        \n",
    "        self.negative_keys,self.negative_values = self.get_distribution()\n",
    "        \n",
    "        self.lbls = torch.cat(\n",
    "            (torch.ones(batch_size,2,dtype=torch.float64),\n",
    "            torch.zeros(batch_size,k,dtype=torch.float64))\n",
    "            ,dim=1)\n",
    "        \n",
    "    def get_distribution(self):\n",
    "        occurance_dictionary = {}\n",
    "        tokens = self.tokens.tolist()\n",
    "        for token in tokens:\n",
    "            occurance_dictionary[token] = occurance_dictionary.get(token,0)+1\n",
    "\n",
    "        values = list(occurance_dictionary.values())\n",
    "        keys = list(occurance_dictionary.keys())\n",
    "\n",
    "        values = [value**(3/4) for value in values]\n",
    "        values = [value/sum(values) for value in values]\n",
    "        \n",
    "        values = torch.tensor(values)\n",
    "        keys = torch.tensor(keys)\n",
    "        \n",
    "        return keys,values\n",
    "    \n",
    "    def get_targets_negative(self):\n",
    "        \n",
    "        self.ids = torch.multinomial(self.negative_values,(self.batch_size*self.k)\n",
    "                                     ,replacement=True).view(self.batch_size,self.k)\n",
    "        \n",
    "        targets_negative = self.negative_keys[self.ids]\n",
    "        \n",
    "        return targets_negative\n",
    "    \n",
    "    def get_context_ids(self):\n",
    "        \n",
    "        ids_context = torch.randint(1,self.num_tokens-1,(self.batch_size,1))\n",
    "        ids_targets_positive = torch.cat((ids_context-1,ids_context+1),dim=1)\n",
    "        \n",
    "        return ids_context,ids_targets_positive\n",
    "    \n",
    "    def load_batch(self):\n",
    "        \n",
    "        targets_negative = self.get_targets_negative()\n",
    "        ids_context,ids_targets_positive = self.get_context_ids()\n",
    "        \n",
    "        context = self.tokens[ids_context]\n",
    "        targets_positive = self.tokens[ids_targets_positive]\n",
    "        \n",
    "        targets = torch.cat((targets_positive,targets_negative),dim=-1)\n",
    "                    \n",
    "        return (context,targets,self.lbls),(ids_context,ids_targets_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 20\n",
      "k = 10\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "k = 10\n",
    "loader = Loader(tokens,batch_size,k)\n",
    "\n",
    "print(f'{batch_size = }')\n",
    "print(f'{k = }')\n",
    "\n",
    "(context,targets,lbls),_ = loader.load_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,n_vocabs):\n",
    "        super(Model,self).__init__()\n",
    "        self.embeddings = nn.Embedding(n_vocabs,1024)\n",
    "        self.thetas = nn.Embedding(n_vocabs,1024)\n",
    "    \n",
    "    def forward(self,context,targets):\n",
    "        emb_context = self.embeddings(context)\n",
    "        theta_targets = self.thetas(targets)\n",
    "        outputs = torch.matmul(theta_targets,emb_context.permute(0,2,1)).to(torch.float64)\n",
    "        predictions = F.sigmoid(outputs).squeeze(-1)\n",
    "        return emb_context,theta_targets,predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(len(tok.vocabs))\n",
    "model.load_state_dict(torch.load('embedding_model_state_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "k = 10\n",
    "loader = Loader(tokens,batch_size,k)\n",
    "loss_fn = nn.BCELoss()\n",
    "lr = 100\n",
    "lr_decay_rate = 0.9\n",
    "optimizer = optim.SGD(model.parameters(),lr=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7164157067651237\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    (context,targets,lbls),_ = loader.load_batch()\n",
    "    emb_context,theta_targets,preds = model(context,targets)\n",
    "    loss = loss_fn(preds,lbls)\n",
    "    print(loss.item())\n",
    "    return preds\n",
    "preds = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0.], dtype=torch.float64,\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds[preds>0.2] = 1\n",
    "# preds[preds!=1] = 0\n",
    "preds[5,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(10000)):\n",
    "    context,targets,lbls = loader.load_batch()\n",
    "    \n",
    "    context = torch.tensor(context)\n",
    "    targets = torch.tensor(targets)\n",
    "    lbls = torch.tensor(lbls)\n",
    "    \n",
    "    emb_context,theta_targets,predictions = model(context,targets)\n",
    "    loss = loss_fn(predictions,lbls)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if i%500 == 0:\n",
    "        print(loss.item())\n",
    "        optimizer.param_groups[0]['lr'] *= lr_decay_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch-cuda]",
   "language": "python",
   "name": "conda-env-torch-cuda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
